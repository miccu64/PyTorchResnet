{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    Sequential,\n",
    "    Conv2d,\n",
    "    ReLU,\n",
    "    ModuleList,\n",
    "    BatchNorm2d,\n",
    "    ReLU,\n",
    "    Linear,\n",
    "    Flatten,\n",
    "    AdaptiveAvgPool2d,\n",
    "    Softmax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, is_train: bool):\n",
    "        all_transforms = [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "        if is_train:\n",
    "            all_transforms = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4)] + all_transforms\n",
    "\n",
    "        dataset = CIFAR10(root=\"data\", download=True, train=is_train, transform=transforms.Compose(all_transforms))\n",
    "        dataset.data = dataset.data\n",
    "        dataset.targets = dataset.targets\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=1024)\n",
    "\n",
    "        self.data = torch.cat([X for X, _ in dataloader])\n",
    "        self.labels = torch.eye(10)[torch.cat([y for _, y in dataloader])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_dataset = CIFAR10Dataset(True)\n",
    "test_dataset = CIFAR10Dataset(False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetConvSizes:\n",
    "    def __init__(self, resnet_layers: int, block_size: int, conv2: int, conv3: int, conv4: int, conv5) -> None:\n",
    "        if block_size != 2 and block_size != 3:\n",
    "            raise ValueError(f\"Possible block sizes are [2, 3]. Provided: {block_size}\")\n",
    "        if resnet_layers < 5:\n",
    "            raise ValueError(f\"Possible lowest layers number: 5. Provided: {resnet_layers}\")\n",
    "        if any(value < 1 for value in (conv2, conv3, conv4)) or conv5 < 0:\n",
    "            raise ValueError(\"Wrong layers count\")\n",
    "\n",
    "        self.resnet_layers = resnet_layers\n",
    "        self.block_size = block_size\n",
    "        self.conv2 = conv2\n",
    "        self.conv3 = conv3\n",
    "        self.conv4 = conv4\n",
    "        self.conv5 = conv5\n",
    "\n",
    "        size = self.layers_count()\n",
    "        if size != resnet_layers:\n",
    "            raise ValueError(f\"Wrong summary ResNet size. Current: {size}, expected: {resnet_layers}\")\n",
    "\n",
    "    def layers_count(self) -> int:\n",
    "        return ((self.conv2 + self.conv3 + self.conv4 + self.conv5) * self.block_size) + 2\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"ConvSize(resnet_layers={self.resnet_layers}, block_size={self.block_size}, conv2={self.conv2}, conv3={self.conv3}, conv4={self.conv4}, conv5={self.conv5})\"\n",
    "\n",
    "\n",
    "class ShortcutTypeEnum(Enum):\n",
    "    Padding = 1\n",
    "    Convolution = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingLayer(Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super(PaddingLayer, self).__init__()\n",
    "        self.pad = (0, 0, 0, 0, in_channels // 2, in_channels // 2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.nn.functional.pad(x[:, :, ::2, ::2], pad=self.pad, mode=\"constant\", value=0.0)\n",
    "\n",
    "\n",
    "class ResNetModule(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_sizes: ResNetConvSizes,\n",
    "        shortcut_type: ShortcutTypeEnum = ShortcutTypeEnum.Padding,\n",
    "        batchnorm_momentum: float = 0.1,\n",
    "    ):\n",
    "        super(ResNetModule, self).__init__()\n",
    "\n",
    "        if batchnorm_momentum <= 0 or batchnorm_momentum >= 1:\n",
    "            raise ValueError(f\"Momentum must be value between (0, 1). Provided: {batchnorm_momentum}\")\n",
    "\n",
    "        self.conv_sizes = conv_sizes\n",
    "        self.momentum = batchnorm_momentum\n",
    "        self.shortcut_type = shortcut_type\n",
    "\n",
    "        self.initial_channels = self.latest_channels = 16\n",
    "\n",
    "        self.conv1 = Sequential(\n",
    "            # bias is redundant when using batch normalization\n",
    "            Conv2d(3, self.latest_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            BatchNorm2d(self.latest_channels, momentum=self.momentum),\n",
    "            ReLU()\n",
    "            # no pooling there\n",
    "        ).apply(self.__init_weights)\n",
    "\n",
    "        self.conv2 = self.__create_blocks(conv_sizes.conv2)\n",
    "        self.conv3 = self.__create_blocks(conv_sizes.conv3)\n",
    "        self.conv4 = self.__create_blocks(conv_sizes.conv4)\n",
    "        self.conv5 = self.__create_blocks(conv_sizes.conv5)\n",
    "\n",
    "        self.relu = ReLU()\n",
    "        self.shortcuts = self.__create_shortcuts()\n",
    "\n",
    "        self.output = Sequential(AdaptiveAvgPool2d((1, 1)), Flatten(), Linear(self.latest_channels, 10), Softmax(dim=1))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: str):\n",
    "        return torch.load(path)\n",
    "\n",
    "    def save_model(self, path: str) -> None:\n",
    "        torch.save(self, path)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        all_convs = [self.conv2, self.conv3, self.conv4]\n",
    "        if self.conv_sizes.conv5 > 0:\n",
    "            all_convs.append(self.conv5)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        for index, conv_blocks in enumerate(all_convs):\n",
    "            previous_x = self.shortcuts[index](x)\n",
    "            for block in conv_blocks:\n",
    "                x = block(x) + previous_x\n",
    "                previous_x = x.clone()\n",
    "                x = self.relu(x)\n",
    "\n",
    "        return self.output(x)\n",
    "\n",
    "    def __create_blocks(self, conv_size: int) -> ModuleList:\n",
    "        modules = ModuleList()\n",
    "        if conv_size == 0:\n",
    "            return modules\n",
    "\n",
    "        create_block = self.__create_basic_block if self.conv_sizes.block_size == 2 else self.__create_bottleneck_block\n",
    "\n",
    "        modules.append(create_block(self.latest_channels, True))\n",
    "        self.latest_channels *= 2\n",
    "\n",
    "        for _ in range(1, conv_size):\n",
    "            modules.append(create_block(self.latest_channels))\n",
    "\n",
    "        return modules.apply(self.__init_weights)\n",
    "\n",
    "    def __init_weights(self, module):\n",
    "        if isinstance(module, (Conv2d, Linear)):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "    def __create_basic_block(self, in_channels: int, downsample_dimensions: bool = False) -> Sequential:\n",
    "        if not self.__is_power_of_2(in_channels):\n",
    "            raise ValueError(\"Input channels number is not power of 2\")\n",
    "\n",
    "        first_stride = 1\n",
    "        out_channels = in_channels\n",
    "\n",
    "        if downsample_dimensions:\n",
    "            first_stride *= 2\n",
    "            out_channels *= 2\n",
    "\n",
    "        return Sequential(\n",
    "            Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=first_stride, bias=False),\n",
    "            BatchNorm2d(out_channels, momentum=self.momentum),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            BatchNorm2d(out_channels, momentum=self.momentum),\n",
    "        )\n",
    "\n",
    "    def __create_bottleneck_block(self, in_channels: int, downsample_dimensions: bool = False) -> Sequential:\n",
    "        first_stride = 1\n",
    "        internal_channels = in_channels // 4\n",
    "        out_channels = in_channels\n",
    "\n",
    "        if downsample_dimensions:\n",
    "            first_stride *= 2\n",
    "            internal_channels *= 2\n",
    "            out_channels *= 2\n",
    "\n",
    "        if not all(self.__is_power_of_2(num) for num in [in_channels, internal_channels, out_channels]):\n",
    "            raise ValueError(\"Channels number is not power of 2\")\n",
    "\n",
    "        return Sequential(\n",
    "            Conv2d(in_channels, internal_channels, padding=0, kernel_size=1, stride=first_stride, bias=False),\n",
    "            BatchNorm2d(internal_channels, momentum=self.momentum),\n",
    "            ReLU(),\n",
    "            Conv2d(internal_channels, internal_channels, padding=1, kernel_size=3, stride=1, bias=False),\n",
    "            BatchNorm2d(internal_channels, momentum=self.momentum),\n",
    "            ReLU(),\n",
    "            Conv2d(internal_channels, out_channels, padding=0, kernel_size=1, stride=1, bias=False),\n",
    "            BatchNorm2d(out_channels, momentum=self.momentum),\n",
    "        )\n",
    "\n",
    "    def __create_shortcuts(self) -> ModuleList:\n",
    "        in_channels = self.initial_channels\n",
    "        shortcuts = ModuleList()\n",
    "\n",
    "        iters = 4 if self.conv_sizes.conv5 > 0 else 3\n",
    "        for _ in range(iters):\n",
    "            match self.shortcut_type:\n",
    "                case ShortcutTypeEnum.Convolution:\n",
    "                    out_channels = in_channels * 2\n",
    "                    seq = Sequential(\n",
    "                        Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False),\n",
    "                        BatchNorm2d(out_channels, momentum=self.momentum),\n",
    "                    ).apply(self.__init_weights)\n",
    "                    shortcuts = shortcuts.append(seq)\n",
    "\n",
    "                case ShortcutTypeEnum.Padding:\n",
    "                    shortcuts = shortcuts.append(Sequential(PaddingLayer(in_channels)))\n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(\"Not supported shortcut type\")\n",
    "\n",
    "            in_channels *= 2\n",
    "\n",
    "        return shortcuts\n",
    "\n",
    "    def __is_power_of_2(self, n: int) -> bool:\n",
    "        return (n & (n - 1) == 0) and n != 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.conv_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "def find_optimal_parameters(conv_sizes: list[ResNetConvSizes]) -> dict:\n",
    "    net = NeuralNetRegressor(\n",
    "        module=ResNetModule, optimizer=torch.optim.SGD, criterion=torch.nn.MSELoss(), device=device, verbose=1,\n",
    "        callbacks=[('early_stopping', EarlyStopping(patience=5))]\n",
    "    )\n",
    "    params = {\n",
    "        \"max_epochs\": [4],\n",
    "        \"batch_size\": [128],\n",
    "        \"optimizer__momentum\": [0.85, 0.9],\n",
    "        \"optimizer__lr\": [0.1, 0.01],\n",
    "        \"optimizer__weight_decay\": [10**-5, 10**-4],\n",
    "        \"module__conv_sizes\": conv_sizes,\n",
    "        \"module__shortcut_type\": [ShortcutTypeEnum.Padding],\n",
    "        \"module__batchnorm_momentum\": [0.1],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(net, params, refit=False, scoring=\"neg_mean_squared_error\", cv=3, n_jobs=4, verbose=1)\n",
    "\n",
    "    samples = 200\n",
    "    grid_search.fit(train_dataset.data[:samples], train_dataset.labels[:samples])\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_mse = -grid_search.best_score_\n",
    "    print(\"Best Mean Squared Error:\", best_mse)\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best conv_sizes:\", best_params[\"module__conv_sizes\"])\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    }
   ],
   "source": [
    "resnet18_conv_sizes = [ResNetConvSizes(18, 2, 2, 4, 2, 0), ResNetConvSizes(18, 2, 2, 2, 2, 2)]\n",
    "resnet18_best_params = find_optimal_parameters(resnet18_conv_sizes)\n",
    "\n",
    "resnet18_conv = resnet18_best_params[\"module__conv_sizes\"]\n",
    "print(resnet18_best_params)\n",
    "print(resnet18_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model: ResNetModule):\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels = torch.argmax(labels,dim=1).to(device)\n",
    "\n",
    "            predictions = torch.argmax(model(data),dim=1)\n",
    "            correct += (predictions == labels).sum()\n",
    "\n",
    "        accuracy = float(correct)/float(len(test_dataset.data)) * 100\n",
    "        print(model)\n",
    "        print(f\"Accuracy of the model: {accuracy}%\")\n",
    "\n",
    "def train(model: ResNetModule):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.0001)\n",
    "\n",
    "    model.train()\n",
    "    loss_SGD = 0\n",
    "    iterations = 200\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        for x, y in train_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss_SGD = loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass for computing the gradients of the loss \n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "        print(f\"Iteration: {i}, loss: {loss_SGD:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss: 0.3201387822628021\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m resnet18 \u001b[39m=\u001b[39m ResNetModule(ResNetConvSizes(\u001b[39m18\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m), ShortcutTypeEnum\u001b[39m.\u001b[39mPadding)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train(resnet18)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#resnet18.forward(train_dataset.data[:10].to(device))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#summary(resnet18, (3, 32, 32))\u001b[39;00m\n",
      "\u001b[1;32m/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# calculating the loss between original and predicted data points\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m previous_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcuts[index](x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m conv_blocks:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     x \u001b[39m=\u001b[39m block(x) \u001b[39m+\u001b[39m previous_x\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     previous_x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mclone()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/linux/PyTorchResnet/PyTorchResnet/pytorch_resnet.ipynb#X12sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet18 = ResNetModule(ResNetConvSizes(18, 2, 2, 4, 2, 0), ShortcutTypeEnum.Padding).to(device)\n",
    "train(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvSize(resnet_layers=18, block_size=2, conv2=2, conv3=4, conv4=2, conv5=0)\n",
      "Accuracy of the model: 18.32%\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvSize(resnet_layers=18, block_size=2, conv2=2, conv3=4, conv4=2, conv5=0)\n",
      "Accuracy of the model: 51.88%\n"
     ]
    }
   ],
   "source": [
    "resnet18 = ResNetModule.load_model('resnet18-2-2-4-2-0.model')\n",
    "check_accuracy(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTIONS:\n",
    "# - is it good idea to perform GridCV? If yes, any faster solution (NeuralNetRegressor?)? Or which parameters aren't necessary to improve?\n",
    "# - any possibility to improve training time? Batch size is in paper 128... (20 mins for ResNet18 in Google Colab is long)\n",
    "# - should I show some plots of loss or sth else? What plots would be nice?\n",
    "# - should stop learning earlier if loss is almost constant?\n",
    "\n",
    "# TODO:\n",
    "# - follow paper in changing lr\n",
    "\n",
    "# TODO ideas:\n",
    "# - move training method to model\n",
    "# - allow training continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
