{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, is_train: bool):\n",
    "        all_transforms = [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "        if is_train:\n",
    "            all_transforms = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4)] + all_transforms\n",
    "\n",
    "        dataset = CIFAR10(root=\"data\", download=True, train=is_train, transform=transforms.Compose(all_transforms))\n",
    "        dataset.data = dataset.data[:100]\n",
    "        dataset.targets = dataset.targets[:100]\n",
    "\n",
    "        dataloader = DataLoader(dataset)\n",
    "\n",
    "        self.data = torch.cat([X for X, _ in dataloader])\n",
    "        self.labels = torch.eye(10)[torch.cat([y for _, y in dataloader])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_dataset = CIFAR10Dataset(True)\n",
    "test_dataset = CIFAR10Dataset(False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetConvSizes:\n",
    "    def __init__(self, resnet_layers: int, block_size: int, conv2: int, conv3: int, conv4: int, conv5: int = 0) -> None:\n",
    "        if block_size != 2 and block_size != 3:\n",
    "            raise ValueError(f\"Possible block sizes are [2, 3]. Provided: {block_size}\")\n",
    "        if resnet_layers < 5:\n",
    "            raise ValueError(f\"Possible lowest layers number: 5. Provided: {resnet_layers}\")\n",
    "        if any(value < 1 for value in (conv2, conv3, conv4)) or conv5 < 0:\n",
    "            raise ValueError(\"Wrong layers count\")\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.conv2 = conv2\n",
    "        self.conv3 = conv3\n",
    "        self.conv4 = conv4\n",
    "        self.conv5 = conv5\n",
    "\n",
    "        size = self.layers_count()\n",
    "        if size != resnet_layers:\n",
    "            raise ValueError(f\"Wrong summary ResNet size. Current: {size}, expected: {resnet_layers}\")\n",
    "\n",
    "    def layers_count(self) -> int:\n",
    "        return ((self.conv2 + self.conv3 + self.conv4 + self.conv5) * self.block_size) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from torch import Tensor\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    Sequential,\n",
    "    Conv2d,\n",
    "    ReLU,\n",
    "    ModuleList,\n",
    "    BatchNorm2d,\n",
    "    ReLU,\n",
    "    Linear,\n",
    "    Flatten,\n",
    "    AdaptiveAvgPool2d,\n",
    "    Softmax,\n",
    ")\n",
    "\n",
    "\n",
    "class ShortcutTypeEnum(Enum):\n",
    "    Convolution = 1\n",
    "    Padding = 2\n",
    "\n",
    "\n",
    "class ResNetModule(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_sizes: ResNetConvSizes,\n",
    "        shortcut_type: ShortcutTypeEnum = ShortcutTypeEnum.Convolution,\n",
    "        momentum: float = 0.9,\n",
    "    ):\n",
    "        super(ResNetModule, self).__init__()\n",
    "\n",
    "        if momentum <= 0 or momentum >= 1:\n",
    "            raise ValueError(f\"Momentum must be value between (0, 1). Provided: {momentum}\")\n",
    "\n",
    "        self.conv_sizes = conv_sizes\n",
    "        self.momentum = momentum\n",
    "        self.shortcut_type = shortcut_type\n",
    "\n",
    "        self.latest_channels = 16\n",
    "\n",
    "        self.conv1 = Sequential(\n",
    "            # bias is redundant when using batch normalization\n",
    "            Conv2d(3, self.latest_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            BatchNorm2d(self.latest_channels, momentum=self.momentum),\n",
    "            ReLU()\n",
    "            # no pooling there\n",
    "        ).apply(self.__init_weights)\n",
    "\n",
    "        self.conv2 = self.__create_blocks(conv_sizes.conv2)\n",
    "        self.conv3 = self.__create_blocks(conv_sizes.conv3)\n",
    "        self.conv4 = self.__create_blocks(conv_sizes.conv4)\n",
    "        self.conv5 = self.__create_blocks(conv_sizes.conv5)\n",
    "\n",
    "        self.output = Sequential(AdaptiveAvgPool2d((1, 1)), Flatten(), Linear(self.latest_channels, 10), Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.conv1(x)\n",
    "        previous_x = x.clone()\n",
    "\n",
    "        for conv in [self.conv2, self.conv3, self.conv4, self.conv5]:\n",
    "            x, previous_x = self.__forward_conv(conv, x, previous_x)\n",
    "\n",
    "        return self.output(x)\n",
    "\n",
    "    def __forward_conv(self, conv_blocks: ModuleList, x: Tensor, previous_x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        if len(conv_blocks) < 1:\n",
    "            return x, previous_x\n",
    "\n",
    "        shortcut_previous_x = self.__execute_shortcut(previous_x)\n",
    "        for block in conv_blocks:\n",
    "            x = block(x) + shortcut_previous_x\n",
    "            shortcut_previous_x = x.clone()\n",
    "\n",
    "        return x, shortcut_previous_x\n",
    "\n",
    "    def __execute_shortcut(self, previous_x: Tensor):\n",
    "        in_channels = previous_x.shape[1]\n",
    "        shortcut_previous_x = previous_x.clone()\n",
    "\n",
    "        match self.shortcut_type:\n",
    "            case ShortcutTypeEnum.Convolution:\n",
    "                conv = Conv2d(in_channels, in_channels * 2, kernel_size=1, stride=2, bias=False).apply(\n",
    "                    self.__init_weights\n",
    "                )\n",
    "                shortcut_previous_x = conv(shortcut_previous_x)\n",
    "            case ShortcutTypeEnum.Padding:\n",
    "                pad = (0, 0, 0, 0, in_channels // 2, in_channels // 2)\n",
    "                shortcut_previous_x = torch.nn.functional.pad(\n",
    "                    shortcut_previous_x[:, :, ::2, ::2], pad=pad, mode=\"constant\", value=0.0\n",
    "                )\n",
    "            case _:\n",
    "                raise ValueError(\"Not supported shortcut type\")\n",
    "\n",
    "        return shortcut_previous_x\n",
    "\n",
    "    def __create_blocks(self, conv_size: int) -> ModuleList:\n",
    "        modules = ModuleList()\n",
    "        if conv_size == 0:\n",
    "            return modules\n",
    "\n",
    "        create_block = self.__create_basic_block if self.conv_sizes.block_size == 2 else self.__create_bottleneck_block\n",
    "\n",
    "        modules.append(create_block(self.latest_channels, True))\n",
    "        self.latest_channels *= 2\n",
    "\n",
    "        for _ in range(1, conv_size):\n",
    "            modules.append(create_block(self.latest_channels))\n",
    "\n",
    "        return modules.apply(self.__init_weights)\n",
    "\n",
    "    def __init_weights(self, module):\n",
    "        if isinstance(module, (Conv2d, Linear)):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "    def __create_basic_block(self, in_channels: int, downsample_dimensions: bool = False) -> Sequential:\n",
    "        if not self.__is_power_of_2(in_channels):\n",
    "            raise ValueError(\"Input channels number is not power of 2\")\n",
    "\n",
    "        first_stride = 1\n",
    "        out_channels = in_channels\n",
    "        if downsample_dimensions:\n",
    "            first_stride = 2\n",
    "            out_channels = out_channels * 2\n",
    "\n",
    "        return Sequential(\n",
    "            Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=first_stride, bias=False),\n",
    "            BatchNorm2d(out_channels, momentum=self.momentum),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            BatchNorm2d(out_channels, momentum=self.momentum),\n",
    "        )\n",
    "\n",
    "    def __create_bottleneck_block(self, in_channels: int, downsample_dimensions: bool = False) -> Sequential:\n",
    "        first_stride = 1\n",
    "        internal_channels = in_channels // 4\n",
    "        out_channels = in_channels\n",
    "        if downsample_dimensions:\n",
    "            first_stride *= 2\n",
    "            internal_channels *= 2\n",
    "            out_channels *= 2\n",
    "\n",
    "        if not all(self.__is_power_of_2(num) for num in [in_channels, internal_channels, out_channels]):\n",
    "            raise ValueError(\"Channels number is not power of 2\")\n",
    "\n",
    "        return Sequential(\n",
    "            Conv2d(in_channels, internal_channels, padding=1, kernel_size=1, stride=first_stride, bias=False),\n",
    "            BatchNorm2d(internal_channels, momentum=self.momentum),\n",
    "            ReLU(),\n",
    "            Conv2d(internal_channels, internal_channels, padding=1, kernel_size=3, stride=1, bias=False),\n",
    "            BatchNorm2d(internal_channels, momentum=self.momentum),\n",
    "            ReLU(),\n",
    "            Conv2d(internal_channels, out_channels, padding=1, kernel_size=1, stride=1, bias=False),\n",
    "            BatchNorm2d(out_channels, momentum=self.momentum),\n",
    "        )\n",
    "\n",
    "    def __is_power_of_2(self, n: int) -> bool:\n",
    "        return (n & (n - 1) == 0) and n != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = ResNetConvSizes(14,2,2,2,2)\n",
    "module = ResNetModule(sizes, shortcut_type=ShortcutTypeEnum.Padding)\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    module.forward(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def find_optimal_parameters(conv_sizes: list[ResNetConvSizes]):\n",
    "    net = NeuralNetRegressor(\n",
    "        module=ResNetModule, optimizer=torch.optim.SGD, criterion=torch.nn.MSELoss(), device=device, verbose=0\n",
    "    )\n",
    "\n",
    "    lr_and_weight_decay_list = [10**num for num in range(-5, 0)]\n",
    "\n",
    "    params = {\n",
    "        \"lr\": lr_and_weight_decay_list,\n",
    "        \"max_epochs\": [range(2, 9, 2)],\n",
    "        \"batch_size\": [128, 256, 512, 1024],\n",
    "        \"optimizer__weight_decay\": lr_and_weight_decay_list,\n",
    "        \"module__conv_sizes\": conv_sizes,\n",
    "        \"module__shortcut_type\": [ShortcutTypeEnum.Convolution, ShortcutTypeEnum.Padding],\n",
    "        \"module__momentum\": [range(0.1, 1, 0.2)],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(net, params, refit=True, verbose=0, scoring=\"neg_mean_squared_error\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list[2, 4, 6, 8, 10, 12, 14]\n"
     ]
    }
   ],
   "source": [
    "a = list[*range(2, 16, 2)]\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
